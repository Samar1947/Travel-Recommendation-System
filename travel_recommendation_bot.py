# -*- coding: utf-8 -*-
"""Travel Recommendation Bot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V2aUO8tdSuwQ2fk8wmBa2wNLegHgxThI
"""

!pip install -q --upgrade pip
!pip install -q gradio pinecone-client==3.0.0 sentence-transformers transformers pandas

import gradio as gr
from pinecone import Pinecone, ServerlessSpec

import pandas as pd
from sentence_transformers import SentenceTransformer
from transformers import pipeline

print("âœ… All packages installed successfully!")

from google.colab import files
uploaded = files.upload()

# ğŸ”´ Replace your Pinecone API Key below:
pc = Pinecone(api_key="pcsk_4ZpWLj_GBTJtgbjKr9TJXCn1PgYTpkBK55htQSoS3xjGFk7KjihDjVspFjiekFMnXBsVVM")
index_name = "travel-recommendation-index"

# Create index if it doesn't exist
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=384,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

index = pc.Index(index_name)

print("âœ… Pinecone initialized and index ready!")

# =========================
# 3. Upload and Embed Dataset
# =========================
# ğŸ”µ Upload CSV manually if not uploaded yet
from google.colab import files
uploaded = files.upload()

# ğŸ”µ Load the CSV
travel_data = pd.read_csv("/content/Extended_TravelRecommendations.csv")

# ğŸ”µ Load embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# ğŸ”µ Create embeddings
travel_embeddings = embedding_model.encode(travel_data['travel_preferences'].tolist()).tolist()

# ğŸ”µ Prepare vectors for Pinecone
vectors = [
    {"id": str(i), "values": emb, "metadata": {"destination": travel_data['recommended_destination'][i]}}
    for i, emb in enumerate(travel_embeddings)
]

# ğŸ”µ Upsert (upload) into Pinecone
index.upsert(vectors=vectors)

print("âœ… Dataset embedded and indexed into Pinecone successfully!")

# =========================
# 4. Load RAG Conversational Model
# =========================
rag_model = pipeline("text2text-generation", model="google/flan-t5-base")

print("âœ… RAG conversational model loaded successfully!")

# =========================
# 5. Define Travel Recommendation Function
# =========================
def get_travel_recommendation(user_input):
    input_embedding = embedding_model.encode(user_input).tolist()
    results = index.query(vector=input_embedding, top_k=3, include_metadata=True)

    if results.matches:
        best_match = results.matches[0]
        destination = best_match.metadata.get('destination', 'Unknown Destination')
        confidence = best_match.score

        context = f"User preference: {user_input}"
        generated_response = rag_model(context, max_length=256)[0]['generated_text']

        return f"ğŸ–ï¸ **Recommended Destination:** {destination} (Confidence: {confidence:.2f})\n\nğŸ§³ **Details:** {generated_response}"
    else:
        return "ğŸ˜” Sorry, no matching destination found."

# =========================
# 6. Build and Launch Gradio App
# =========================
iface = gr.Interface(
    fn=get_travel_recommendation,
    inputs=gr.Textbox(lines=2, placeholder="Describe your travel preferences here..."),
    outputs="markdown",
    title="ğŸï¸ Travel Recommendation Chatbot",
    description="Get personalized travel suggestions using AI! ğŸŒ",
    theme="soft",
)

# âœ… Launch with share=True for Public URL
iface.launch(share=True)

